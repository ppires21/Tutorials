# Terraform + AWS â€” Endâ€‘toâ€‘End Data Pipeline (Glue Workflow + Lambda + S3 + DynamoDB)

This tutorial reverseâ€‘engineers and explains a **complete Terraform solution** that builds a small **data pipeline** on AWS:

- **S3** bucket for data and code (Glue scripts, Lambda package)
- **Glue Workflow** with two **Glue Jobs** (CSV â†’ Parquet, then write status)
- **Lambda** to **start** the workflow and register the run
- **DynamoDB** to **track run status** (`Started` â†’ `Finished`)
- IAM roles and permissions for **least privilege**

It is written to be **readable for beginners** but **precise enough** for advanced users. Every Terraform block is shown and explained.

---

## Project Layout (from the example)

```
terraform-challenge/
â”œâ”€â”€ backend.tf
â”œâ”€â”€ main.tf
â”œâ”€â”€ variables.tf
â”œâ”€â”€ outputs.tf
â”œâ”€â”€ terraform.tfvars
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input-data.csv
â”œâ”€â”€ glue/
â”‚   â”œâ”€â”€ job1.py
â”‚   â””â”€â”€ job2.py
â””â”€â”€ lambda/
    â””â”€â”€ lambda_function.py
```

> The `.terraform/` directory and lockfiles are generated by Terraform and are **not** written by hand.

---

## ðŸ”§ Provider & Remote State Backend

### Provider configuration
We tell Terraform **which cloud** to use and in which **region/profile** to operate.

```hcl
provider "aws" {
  region  = var.region
  profile = var.profile
}
```

**Explanation**  
- `region` and `profile` come from variables (`var.region`, `var.profile`). Using variables keeps the code portable across environments and AWS profiles.

### (Optional) Remote state backend (S3)
Storing state remotely enables **team collaboration** and state **locking**.

```hcl
# ---------------------------------------------------------------------------------------------------------------------
# Add your backend configurations from exercise 2 here
# ---------------------------------------------------------------------------------------------------------------------

# Terraform backend remote configuration.
# Only uncomment this block after the bucket was created.
#
terraform {
  backend "s3" {
    region  = "eu-central-1"
    profile = "data-academy"
    bucket         = "iac-qxz6jly"
    key            = "terraform.tfstate"
    encrypt        = true
    use_lockfile   = true
  }
}
```

**Explanation**  
- `backend "s3"` tells Terraform to keep `terraform.tfstate` in an S3 bucket instead of locally.  
- `key` is the object path inside the bucket.  
- `use_lockfile = true` uses DynamoDBâ€‘based locking under the hood (Terraform 1.9+).  
- Only enable this **after** the bucket exists (the file warns you about that).

---

## Inputs (variables) and Defaults

```hcl
variable "region" {
  description = "The AWS region to deploy into (e.g. eu-central-1)."
  type        = string
  default     = "eu-central-1"
}

variable "profile" {
  description = "The profile for the deploys"
  type        = string
  default     = "data-academy"
}

variable "bucket_name" {
  description = "S3 Bucket for Data and Scripts"
  type        = string
  default     = "challenge-qxz6jly-pedropires"        
}

variable "glue_job1_name" {
  description = "Name for Glue Job #1"
  type        = string
  default     = "challenge-glue-job1-ctw04557"
}

variable "glue_job2_name" {
  description = "Name for Glue Job #2"
  type        = string
  default     = "challenge-glue-job2-ctw04557"
}

variable "lambda_function_name" {
  description = "Name of the Lambda function that triggers the workflow"
  type        = string
  default     = "challenge_lambda_function-ctw04557"
}

variable "dynamodb_table_name" {
  description = "DynamoDB table to track workflow status"
  type        = string
  default     = "challenge_dynamo_table-ctw04557"
}
```

**Explanation**  
- Variables include **region**, **profile**, names for S3 bucket, IAM role, Glue jobs, Lambda function, and DynamoDB table.  
- Sensible defaults are provided (useful for demos), but you can override them in `terraform.tfvars` or via CLI (`-var`).

Example values used in the project:

```hcl
profile = "data-academy"
region="eu-central-1"
#environment = challenge-demo
bucket_name="challenge-qxz6jly-pedropires"
```

---

## S3: Bucket and Objects

The pipeline stores data and code in S3:
- CSV input at `s3://<bucket>/data/input-data.csv`
- Glue scripts at `s3://<bucket>/glue/job1.py` and `.../job2.py`
- Lambda deployment ZIP at `s3://<bucket>/lambda.zip`

### Bucket
```hcl
resource "aws_s3_bucket" "challenge_bucket" {
  bucket = var.bucket_name
  force_destroy = true
}
```

**Explanation**  
- `force_destroy = true` allows Terraform to delete the bucket even if itâ€™s nonâ€‘empty (handy for demos â€” caution in prod!).

### Upload input data and scripts
```hcl
resource "aws_s3_object" "input_data" {
  bucket = aws_s3_bucket.challenge_bucket.id
  key    = "data/input-data.csv"
  source = "${path.module}/data/input-data.csv" 
  etag   = filemd5("${path.module}/data/input-data.csv")
}
```

```hcl
resource "aws_s3_object" "glue_job1_script" {
  bucket = aws_s3_bucket.challenge_bucket.id
  key    = "glue/job1.py"
  source = "${path.module}/glue/job1.py"
  etag   = filemd5("${path.module}/glue/job1.py")
}
```

```hcl
resource "aws_s3_object" "glue_job2_script" {
  bucket = aws_s3_bucket.challenge_bucket.id
  key    = "glue/job2.py"
  source = "${path.module}/glue/job2.py"
  etag   = filemd5("${path.module}/glue/job2.py")
}
```

**Explanation**  
- `aws_s3_object` uploads local files to the bucket path you specify via `key`.  
- This ensures code/data land in S3 **before** jobs/lambda reference them.

### Package and upload Lambda ZIP
Terraform uses the `archive_file` data source to **build a ZIP** from source code, then uploads it.

```hcl
data "archive_file" "lambda_zip" {
  type        = "zip"
  source_file = "${path.module}/lambda/lambda_function.py"
  output_path = "${path.module}/lambda/lambda_function_payload.zip"
}
```

```hcl
resource "aws_s3_object" "lambda_zip" {
  bucket = aws_s3_bucket.challenge_bucket.id
  key    = "lambda/lambda_function_payload.zip"
  source = data.archive_file.lambda_zip.output_path
  etag   = data.archive_file.lambda_zip.output_md5
}
```

**Explanation**  
- `archive_file` zips your local `lambda/` folder; Terraform tracks the **content hash** so updates trigger new uploads.  
- The `aws_s3_object.lambda_zip` then uploads the ZIP to S3 (versioned by ETag/hash implicitly).

---

## IAM: Shared Execution Role + Policies

We use **one role** for Glue Jobs **and** Lambda (simplifies the exercise) and attach a custom policy that grants minimum required access.

```hcl
resource "aws_iam_role" "shared_role" {
  name = "challenge-shared-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = "sts:AssumeRole",
        Principal = {
          Service = [
            "lambda.amazonaws.com",
            "glue.amazonaws.com"
          ]
        }
      }
    ]
  })
}
```

```hcl
resource "aws_iam_role_policy_attachment" "shared_basic_execution" {
  role       = aws_iam_role.shared_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
}
```

```hcl
resource "aws_iam_role_policy" "shared_custom_policy" {
  name = "challenge-shared-custom-policy"
  role = aws_iam_role.shared_role.id

  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = [
          "glue:StartWorkflowRun",
          "glue:StartJobRun",
          "dynamodb:UpdateItem",
          "dynamodb:PutItem",
          "dynamodb:GetItem",
          "s3:GetObject",
          "s3:PutObject",
          "s3:ListBucket",
          "s3:DeleteObject"
        ],
        Resource = ["*"]
      }
    ]
  })
}
```

**Explanation**  
- The **trust policy** allows both `glue.amazonaws.com` and `lambda.amazonaws.com` to assume the role.  
- The **managed policy** `AWSLambdaBasicExecutionRole` provides CloudWatch Logs permissions for Lambda logs.  
- The **custom inline policy** should include explicit allows for:
  - `s3:GetObject`, `s3:PutObject` on the specific bucket/prefixes,
  - `glue:StartWorkflowRun`, `glue:GetJobRun`, etc.,
  - `dynamodb:PutItem`, `dynamodb:UpdateItem` on the specific table,
  - any reads Glue job needs (e.g., `glue:GetTable`, `logs:CreateLogGroup` as applicable).
  Adjust ARNs to your actual resource names for **least privilege**.

---

## Glue Workflow, Jobs, and Triggers

### Workflow
```hcl
resource "aws_glue_workflow" "workflow" {
  name = "ctw04557_challenge_workflow"  # Usa a tua var, mas adapta se quiseres hardcoded
}
```

**Explanation**  
- A **workflow** orchestrates Glue jobs and triggers. Weâ€™ll attach two triggers below.

### Job 1 â€” CSV â†’ Parquet
```hcl
resource "aws_glue_job" "job1" {
  name     = var.glue_job1_name
  role_arn = aws_iam_role.shared_role.arn

  command {
    name            = "glueetl"
    script_location = "s3://${aws_s3_bucket.challenge_bucket.bucket}/glue/job1.py"
    python_version  = "3"
  }

  max_retries = 1
}
```

**Explanation**  
- `role_arn` uses the IAM role above.  
- `command.script_location` points to the **S3 path** for `job1.py`.  
- `default_arguments` may include `--job-language`, `--TempDir`, `--WORKFLOW_RUN_ID` (if passed).

**Script (job1.py)** â€” reads CSV and writes Parquet to S3:

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

# Initialize contexts and job
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

#TODO - Add the bucket name of the bucket you created here
bucket = "s3://challenge-qxz6jly-pedropires"

# Job 1 reads CSV and writes Parquet (caminhos corrigidos para consistency e matching)
input_path = f"{bucket}/data/input-data.csv"
output_path = f"{bucket}/data/job1-output/"


print(f"Reading CSV from: {input_path}")

df = spark.read.option("header", "true").csv(input_path)
df.write.mode("overwrite").parquet(output_path)

print(f"Wrote parquet to: {output_path}")

job.commit()
```

Key ideas:
- Uses Spark via Glue to read CSV input and write Parquet output.  
- Bucket path is hardâ€‘coded in the example; in production youâ€™d pass it as an argument or Environment/Job parameter.

### Job 2 â€” Mark run as Finished in DynamoDB
```hcl
resource "aws_glue_job" "job2" {
  name     = var.glue_job2_name
  role_arn = aws_iam_role.shared_role.arn

  command {
    name            = "glueetl"
    script_location = "s3://${aws_s3_bucket.challenge_bucket.bucket}/glue/job2.py"
    python_version  = "3"
  }

  max_retries = 1
}
```

**Explanation**  
- Also runs with same role.  
- Needs permissions to update DynamoDB.  
- `default_arguments` can pass `--WORKFLOW_RUN_ID` into the script.

**Script (job2.py)** â€” update run status to `Finished`:

```python
import sys

import boto3
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

# Initialize contexts and job
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'WORKFLOW_RUN_ID'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

#TODO - Add the bucket name you created here as well as the dynamodb table name
bucket = "s3://challenge-qxz6jly-pedropires"
dynamo_table_name = "challenge_dynamo_table-ctw04557"

# Job 2 reads Parquet and writes JSON with aggregation (caminhos corrigidos para matching)
input_path = f"{bucket}/data/job1-output/"
output_path = f"{bucket}/data/job2-output/"

print(f"Reading Parquet from: {input_path}")

df = spark.read.parquet(input_path)
filtered_df = df.filter(df.age < 65)
filtered_df.write.mode("overwrite").json(output_path)

print(f"Wrote JSON to: {output_path}")  # Corrigido o print

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table(dynamo_table_name)

workflow_run_id = args['WORKFLOW_RUN_ID']
print(f"RUN ID: {workflow_run_id}")
table.update_item(
    Key={
        'run_id': workflow_run_id  # Agora matching com hash_key da tabela
    },
    UpdateExpression ="set #status = :val",
    ExpressionAttributeValues={
        ":val": "Finished"
    },
    ExpressionAttributeNames={
        "#status": "status"
   }
)

job.commit()
```

### Triggers
- **Trigger 1** starts **Job 1** on demand.
- **Trigger 2** starts **Job 2** **only if** Job 1 **succeeds**.

```hcl
resource "aws_glue_trigger" "trigger_job1" {
  name          = "trigger-job1"
  type          = "ON_DEMAND"
  workflow_name = aws_glue_workflow.workflow.name

  actions {
    job_name = aws_glue_job.job1.name
  }
}
```

```hcl
resource "aws_glue_trigger" "trigger_job2" {
  name          = "trigger-job2"
  type          = "CONDITIONAL"
  workflow_name = aws_glue_workflow.workflow.name

  predicate {
    conditions {
      job_name = aws_glue_job.job1.name
      state    = "SUCCEEDED"
    }
  }

  actions {
    job_name = aws_glue_job.job2.name
  }
}
```

**Explanation**  
- `type = "ON_DEMAND"` means you (or Lambda) can kick it off via the workflow.  
- `type = "CONDITIONAL"` with `predicate` ensures **ordering** (Job 2 waits for Job 1 success).

---

## DynamoDB â€” Run Metadata Table

```hcl
resource "aws_dynamodb_table" "dynamo_table" {
  name           = var.dynamodb_table_name
  hash_key       = "run_id"
  read_capacity  = 1
  write_capacity = 1
 
  attribute {
    name = "run_id"
    type = "S"
  }
}
```

**Explanation**  
- Table has a **partition key** `run_id` (string).  
- We use it to store the workflow run status (`Started` â†’ `Finished`).  
- Throughput is 1/1 for demo â€” scale up in production, or use onâ€‘demand.

---

## Lambda â€” Start the Workflow + Seed Status

### Function code (Python)
```python
import json
import boto3
import datetime
import os


glue = boto3.client('glue')
dynamodb = boto3.resource('dynamodb')
workflow_name = os.environ['WORKFLOW_NAME']
table_name = os.environ['TABLE_NAME']

def lambda_handler(event, context):
    response = glue.start_workflow_run(Name=workflow_name)

    run_id = response['RunId']

    table = dynamodb.Table(table_name)
    table.put_item(Item={
        'workflow_name': workflow_name,
        'run_id': run_id,
        'trigger_time': str(datetime.datetime.now(datetime.UTC)),
        'status': 'Started'
    })

    return {
        'statusCode': 200,
        'body': json.dumps({'message': 'Workflow started', 'run_id': run_id})
    }

```

**What it does**  
1. Calls `glue.start_workflow_run(Name=WORKFLOW_NAME)` and gets a `RunId`.  
2. Writes an item into DynamoDB with `status = "Started"`.  
3. Returns a simple JSON with the `run_id`.

### Terraform for the Lambda
```hcl
resource "aws_lambda_function" "lambda_function" {
  function_name = var.lambda_function_name
  role          = aws_iam_role.shared_role.arn
  handler       = "lambda_function.lambda_handler"
  runtime       = "python3.13"

  s3_bucket = aws_s3_bucket.challenge_bucket.bucket
  s3_key    = aws_s3_object.lambda_zip.key

  environment {
    variables = {
      TABLE_NAME    = aws_dynamodb_table.dynamo_table.name
      S3_BUCKET     = aws_s3_bucket.challenge_bucket.bucket
      WORKFLOW_NAME = aws_glue_workflow.workflow.name
    }
  }
}
```

**Explanation**  
- `s3_bucket` and `s3_key` reference the ZIP we uploaded.  
- `handler` is `lambda_function.lambda_handler` â†’ `<file>.<function>` inside the ZIP.  
- `environment` passes references to **table name**, **bucket**, and **workflow name**.  
- `role` is the shared execution role; adjust to split responsibilities in stricter environments.

---

## Outputs â€” What Terraform Prints After Apply

```hcl
# ---------------------------------------------------------------------------------------------------------------------
# OUTPUTS: Print useful resource names after terraform apply
# ---------------------------------------------------------------------------------------------------------------------

# The S3 bucket that stores all data, scripts, and lambda zip
output "bucket_name" {
  description = "S3 bucket used for data, scripts, and lambda artifacts"
  value       = aws_s3_bucket.challenge_bucket.bucket
}

# The AWS Lambda function that triggers the workflow
output "lambda_function_name" {
  description = "Name of the Lambda function that starts the Glue workflow"
  value       = aws_lambda_function.lambda_function.function_name
}

# The two AWS Glue jobs used in the workflow
output "glue_job_names" {
  description = "Names of the Glue jobs created"
  value       = [
    aws_glue_job.job1.name,
    aws_glue_job.job2.name
  ]
}

# The DynamoDB table that stores workflow status
output "dynamodb_table_name" {
  description = "Name of the DynamoDB table used to track workflow status"
  value       = aws_dynamodb_table.dynamo_table.name
}

```

**Explanation**  
Use these to quickly find deployed resources (bucket, Lambda name, Glue job names, table). They also help external tooling/scripts fetch references.

---

## â–¶ How to Run (Commands + What they do)

```bash
# 1) Validate your AWS auth (profile/credentials)
aws sts get-caller-identity --profile data-academy

# 2) Initialize providers and modules
terraform init

# 3) See what will be created/changed
terraform plan -out plan.out

# 4) Apply the plan (provision resources)
terraform apply plan.out

# 5) (Optional) Reconfigure remote backend in backend.tf once bucket exists, then:
terraform init -migrate-state

# 6) Trigger the Lambda manually (web console or AWS CLI) to start the workflow
#    Or start the Glue workflow run directly via AWS Console/API.
```

**Destroy everything (CAUTION!)**
```bash
terraform destroy
```

---

## ðŸ”’ Security & Production Notes

- **IAM separation**: in production, create **distinct roles** for Glue and Lambda with narrowly scoped policies.  
- **Secrets**: avoid hardâ€‘coding bucket names and table names inside scripts; pass via parameters or environment.  
- **S3 lifecycle**: set lifecycle rules (expiration) for intermediate data (e.g., temporary files).  
- **Observability**: enable CloudWatch logs/metrics for Glue & Lambda; consider tracing with X-Ray.  
- **Idempotency**: reâ€‘runs should not corrupt state; protect keys/paths or use versioned prefixes.  
- **Cost controls**: small read/write capacities and developmentâ€‘class Glue DPUs; tag resources and set budgets.

---

## ðŸ§© Adapting This Template

- Change `bucket_name`, `lambda_function_name`, `dynamodb_table_name` in `terraform.tfvars`.  
- Parameterize **S3 input/output prefixes** (avoid hardâ€‘coding in scripts).  
- Add more Glue jobs to your workflow (e.g., validations, transformations, exports).  
- Add an **API Gateway** â†’ Lambda to expose an HTTP endpoint that starts the workflow.  
- Replace DynamoDB with **Step Functions** or eventâ€‘driven orchestration for larger pipelines.

---

## âœ… Summary

You now have a working, **endâ€‘toâ€‘end** Terraform stack that: **stores data**, **runs ETL**, **tracks status**, and **exposes a trigger** â€” all in AWS with reproducible IaC.

Use this as a foundation for realâ€‘world data platforms and evolve it with environment separation, CI/CD, and tighter security.
